{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SZAftabi/ReQuEST/blob/main/ReQuEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTozikbxD-nM"
      },
      "source": [
        "<p align=\"center\"><font size='5'><b>ReQuEST: </b> </font> <font size='5'><b>Re</b></font>cognizing <font size='5'><b>Qu</b></font>estion <font size='5'><b>E</b></font>ntailment, tag-focused question <font size='5'><b>S</b></font>ummarization, and <font size='5'><b>T</b></font>ag generation\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj_bsm00B5co"
      },
      "source": [
        "ðŸ“„ <b>Paper:</b> <br>\n",
        ">S. Z. Aftabi, S. M. Seyyedi, M. Maleki and S. Farzi, \"<i><b>ReQuEST: A Small-Scale Multi-Task Model for Community Question-Answering Systems</b></i>,\" in IEEE Access, vol. 12, pp. 17137-17151, 2024, [doi: 10.1109/ACCESS.2024.3358287](https://ieeexplore.ieee.org/abstract/document/10413543)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgfOK2hzimpp"
      },
      "source": [
        "## ðŸŒž **Access to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUo-cTD5impq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azuZ4HNximpr"
      },
      "source": [
        "## ðŸŒž **Prerequisites**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rbNs9ItrGBcG"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torchsummary torchviz torchmetrics rouge pytorch_lightning torchvision tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WY7wB8F2FwfT"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import sklearn\n",
        "import sys\n",
        "import datetime\n",
        "import copy\n",
        "import string\n",
        "import transformers\n",
        "import torch\n",
        "import warnings\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d2VXLJpSFyKj"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from typing import Tuple\n",
        "from datetime import timedelta\n",
        "from statistics import mean\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from transformers import BartModel, BartPretrainedModel, BartConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
        "from transformers import logging\n",
        "\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data import Dataset, SequentialSampler, RandomSampler\n",
        "from torchsummary import summary\n",
        "from torchviz import make_dot\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics.classification import Accuracy, F1Score, Precision, Recall\n",
        "from torchmetrics.text import BERTScore\n",
        "from rouge import Rouge\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from tensorboard import notebook\n",
        "\n",
        "from matplotlib.rcsetup import validate_backend\n",
        "from os import truncate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7JINu-limpr"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# ========================= Torch AND GPU =========================\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GtqK87limpu"
      },
      "source": [
        "# ðŸŒž **Proposed Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9-Ib4Ucimpu"
      },
      "source": [
        ">## ðŸ”§ **RQE head**\n",
        "A Neural Network Head for Recognizing Question Entailment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AQM1EQBAimpv"
      },
      "outputs": [],
      "source": [
        "class NN_Model_RQE(nn.Module):\n",
        "    def __init__(self, embed_size, dimensions, do_r):\n",
        "        super(NN_Model_RQE, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = embed_size\n",
        "        for size in dimensions:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_size, size),\n",
        "                nn.LayerNorm(size),\n",
        "                nn.GLU(),\n",
        "                nn.Dropout1d(do_r)\n",
        "            ])\n",
        "            prev_size = size // 2\n",
        "        layers.append(nn.Linear(prev_size, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.network.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "          torch.nn.init.normal_(m.weight, mean=0, std=0.1, generator=torch.Generator().manual_seed(42))\n",
        "          m.bias.data.fill_(0.01)\n",
        "\n",
        "    def forward(self, decoder_last_embd):\n",
        "        return self.network(decoder_last_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P93nuqCwimpv"
      },
      "source": [
        ">## ðŸ”§ **Main Framework**\n",
        "1. **One shared BART encoder +**\n",
        "2. **Two partially shared BART decoders +**\n",
        "2. **Three Neural Network Head** (One for Recognizing Question Entailment, the other one for Query-focused Question Summarization, and the third one for tag generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hTx4pNXTimpv"
      },
      "outputs": [],
      "source": [
        "class Main_Architecture(BartPretrainedModel):\n",
        "  def __init__(self, config: BartConfig, hparams):\n",
        "    super(Main_Architecture, self).__init__(config)\n",
        "\n",
        "    self.learning_rate_Encoder = hparams['lr_Encoder']\n",
        "    self.learning_rate_Decoder = hparams['lr_Decoder']\n",
        "    self.learning_rate_RQE = hparams['lr_RQE']\n",
        "    self.learning_rate_SUM = hparams['lr_SUM']\n",
        "    self.learning_rate_TG = hparams['lr_TG']\n",
        "\n",
        "\n",
        "    self.SUM = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "    self.SUM.lm_head = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").lm_head\n",
        "\n",
        "    self.TG = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "    self.TG.lm_head = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").lm_head\n",
        "    self.TG.model.encoder = self.SUM.model.encoder\n",
        "    self.TG.model.shared = self.SUM.model.shared\n",
        "    self.TG.model.decoder.embed_positions = self.SUM.model.decoder.embed_positions\n",
        "    self.TG.model.decoder.layernorm_embedding = self.SUM.model.decoder.layernorm_embedding\n",
        "    self.TG.model.decoder.embed_tokens = self.SUM.model.decoder.embed_tokens\n",
        "    self.TG.model.decoder.layers = torch.nn.ModuleList(\n",
        "                        [self.SUM.model.decoder.layers[i] if i < 3 else copy.deepcopy(self.SUM.model.decoder.layers[i])\n",
        "                        for i in range(len(self.SUM.model.decoder.layers))])\n",
        "\n",
        "    self.RQE = torch.nn.ModuleDict({\n",
        "            \"encoder\": self.SUM.model.encoder,\n",
        "            \"lm_head\": NN_Model_RQE(hparams['embed_size'], hparams['Dimensions'], hparams['DO_r'],)\n",
        "            })\n",
        "\n",
        "    self.register_buffer(\"final_logits_bias\",\n",
        "                         torch.zeros((1, self.SUM.model.shared.num_embeddings)))\n",
        "\n",
        "  def forward(self,\n",
        "              EncoderRQE_input_ids = None,\n",
        "              EncoderRQE_attention = None,\n",
        "              EncoderSUM_input_ids = None,\n",
        "              EncoderSUM_attention_mask = None,\n",
        "              EncoderTG_input_ids = None,\n",
        "              EncoderTG_attention_mask = None,\n",
        "              DecoderSUM_input_ids = None,\n",
        "              DecoderSUM_attention_mask = None,\n",
        "              DecoderTG_input_ids = None,\n",
        "              DecoderTG_attention_mask = None,\n",
        "              output_attentions = None,\n",
        "              output_hidden_states = None,\n",
        "              encoder_outputs = None,\n",
        "              SUM_labels = None,\n",
        "              TG_labels = None,\n",
        "              past_key_values = None,\n",
        "              head_mask = None,\n",
        "              decoder_head_mask = None,\n",
        "              cross_attn_head_mask = None,\n",
        "              inputs_embeds = None,\n",
        "              decoder_inputs_embeds = None,\n",
        "              use_cache = None,\n",
        "              return_dict = None,\n",
        "              decoder_task = None\n",
        "              ):\n",
        "\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    # ****************** For Summarization *****************\n",
        "    if EncoderSUM_input_ids is not None or\\\n",
        "        DecoderSUM_input_ids is not None or\\\n",
        "          EncoderSUM_attention_mask is not None or\\\n",
        "            DecoderSUM_attention_mask is not None:\n",
        "\n",
        "\n",
        "      BARTOutputs_SUM = self.SUM(input_ids = EncoderSUM_input_ids,\n",
        "                                 attention_mask = EncoderSUM_attention_mask,\n",
        "                                 decoder_input_ids = DecoderSUM_input_ids,\n",
        "                                 decoder_attention_mask = DecoderSUM_attention_mask,\n",
        "                                 encoder_outputs = encoder_outputs,\n",
        "                                 head_mask = head_mask,\n",
        "                                 decoder_head_mask = decoder_head_mask,\n",
        "                                 cross_attn_head_mask = cross_attn_head_mask,\n",
        "                                 past_key_values = past_key_values,\n",
        "                                 inputs_embeds = inputs_embeds,\n",
        "                                 decoder_inputs_embeds = decoder_inputs_embeds,\n",
        "                                 labels = SUM_labels,\n",
        "                                 use_cache = use_cache,\n",
        "                                 output_attentions = output_attentions,\n",
        "                                 output_hidden_states = True,\n",
        "                                 return_dict = True)\n",
        "    # ****************** End Summarization *****************\n",
        "\n",
        "\n",
        "    # ***************** For Tag Genertaion *****************\n",
        "    if EncoderTG_input_ids is not None or\\\n",
        "        DecoderTG_input_ids is not None or\\\n",
        "          EncoderTG_attention_mask is not None or\\\n",
        "            DecoderTG_attention_mask is not None:\n",
        "\n",
        "      BARTOutputs_TG = self.TG(input_ids = EncoderTG_input_ids,\n",
        "                               attention_mask = EncoderTG_attention_mask,\n",
        "                               decoder_input_ids = DecoderTG_input_ids,\n",
        "                               decoder_attention_mask = DecoderTG_attention_mask,\n",
        "                               encoder_outputs = encoder_outputs,\n",
        "                               head_mask = head_mask,\n",
        "                               decoder_head_mask = decoder_head_mask,\n",
        "                               cross_attn_head_mask = cross_attn_head_mask,\n",
        "                               past_key_values = past_key_values,\n",
        "                               inputs_embeds = inputs_embeds,\n",
        "                               decoder_inputs_embeds = decoder_inputs_embeds,\n",
        "                               labels = TG_labels,\n",
        "                               use_cache = use_cache,\n",
        "                               output_attentions = output_attentions,\n",
        "                               output_hidden_states = True,\n",
        "                               return_dict = True)\n",
        "    # ***************** End Tag Genertaion *****************\n",
        "\n",
        "\n",
        "    # ********************** For RQE ***********************\n",
        "    if (EncoderRQE_input_ids != None):\n",
        "\n",
        "      BARTOutputs_RQE = self.RQE.encoder(input_ids = EncoderRQE_input_ids,\n",
        "                                       attention_mask = EncoderRQE_attention,\n",
        "                                       output_hidden_states = True,\n",
        "                                       return_dict = True)\n",
        "      Last_HS_RQE = BARTOutputs_RQE.last_hidden_state\n",
        "      att_mask_expanded = EncoderRQE_attention.unsqueeze(-1).expand(Last_HS_RQE.size())\n",
        "      sum_embeddings = torch.sum(Last_HS_RQE * att_mask_expanded, 1)\n",
        "      sum_mask = att_mask_expanded.sum(1)\n",
        "      sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "      decoder_last_embd = sum_embeddings / sum_mask\n",
        "      Predicted_label = self.RQE.lm_head(decoder_last_embd)\n",
        "    # ********************** End RQE ***********************\n",
        "\n",
        "\n",
        "    if (EncoderRQE_input_ids == None and EncoderTG_input_ids == None):\n",
        "      return BARTOutputs_SUM.logits\n",
        "    elif (EncoderSUM_input_ids == None and EncoderTG_input_ids == None):\n",
        "      return Predicted_label\n",
        "    elif (EncoderSUM_input_ids == None and EncoderRQE_input_ids == None):\n",
        "      return BARTOutputs_TG.logits\n",
        "    else:\n",
        "      return BARTOutputs_SUM.logits, BARTOutputs_TG.logits, Predicted_label,\\\n",
        "       BARTOutputs_SUM.loss, BARTOutputs_TG.loss\n",
        "\n",
        "\n",
        "  def SUM_generate2(self, tokenizer, input_ids, _min_length=5, _max_length=10, _num_beams=3, _no_repeat_ngram_size=3):\n",
        "    self.SUM.eval()\n",
        "    Summary_input_ids = self.SUM.generate(input_ids,\n",
        "                                          max_length = _max_length,\n",
        "                                          min_length = _min_length,\n",
        "                                          num_beams = _num_beams,\n",
        "                                          no_repeat_ngram_size = _no_repeat_ngram_size\n",
        "                                          )\n",
        "    Summary = tokenizer.batch_decode(Summary_input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "    return Summary\n",
        "\n",
        "\n",
        "  def TG_generate2(self, tokenizer, input_ids, _min_length=5, _max_length=10, _num_beams=3, _no_repeat_ngram_size=3):\n",
        "    self.TG.eval()\n",
        "    Tag_input_ids = self.TG.generate(input_ids,\n",
        "                                     max_length = _max_length,\n",
        "                                     min_length = _min_length,\n",
        "                                     num_beams = _num_beams,\n",
        "                                     no_repeat_ngram_size = _no_repeat_ngram_size,\n",
        "                                     )\n",
        "    Tags = tokenizer.batch_decode(Tag_input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "    return Tags\n",
        "\n",
        "\n",
        "  def RQE_predict(self, input_ids, attention_masks):\n",
        "    self.RQE.eval()\n",
        "    sig = nn.Sigmoid()\n",
        "    out = self.RQE.encoder(input_ids = input_ids,  attention_mask = attention_masks, return_dict = True)\n",
        "    LH = out.last_hidden_state\n",
        "    att_mask_expanded = attention_masks.unsqueeze(-1).expand(LH.size())\n",
        "    sum_embeddings = torch.sum(LH * att_mask_expanded, 1)\n",
        "    sum_mask = att_mask_expanded.sum(1)\n",
        "    sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "    dle = sum_embeddings / sum_mask\n",
        "    logits = self.RQE.lm_head(dle)\n",
        "    Pr_labels = torch.round(sig(logits))\n",
        "    return Pr_labels\n",
        "\n",
        "\n",
        "  def Freeze_Parameters(self, FoN_Dec, FoN_Enc):\n",
        "\n",
        "    for param in self.RQE.lm_head.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    # =========== BART Decoder Layers ===========\n",
        "    for i, FON in enumerate(FoN_Dec):\n",
        "      if FON == 0:\n",
        "        for param in self.TG.model.decoder.layers[i].parameters():\n",
        "          param.requires_grad = False\n",
        "        for param in self.SUM.model.decoder.layers[i].parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "    # =========== BART Encoder Layers ===========\n",
        "    for param in self.SUM.model.encoder.embed_positions.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in self.SUM.model.encoder.layernorm_embedding.parameters():\n",
        "        param.requires_grad = False\n",
        "    for i, FON in enumerate(FoN_Enc):\n",
        "      if FON == 0:\n",
        "        for param in self.SUM.model.encoder.layers[i].parameters():\n",
        "          param.requires_grad = False\n",
        "        for param in self.TG.model.encoder.layers[i].parameters():\n",
        "          param.requires_grad = False\n",
        "        for param in self.RQE.encoder.layers[i].parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "\n",
        "  def shift_tokens_right(self, input_ids, pad_token_id, decoder_start_token_id):\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "    if pad_token_id is None:\n",
        "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "    return shifted_input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZdtyJtw7Xp"
      },
      "source": [
        ">## ðŸ”§ **LitReQuEST**\n",
        "ReQuEST with lightning, including training, validation, and test steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mmYwPfGfVXg8"
      },
      "outputs": [],
      "source": [
        "class OverrideEpochStepCallback(Callback):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        pl_module.log(\"step\", trainer.current_epoch + 1)\n",
        "\n",
        "\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        dirpath='/content/drive/MyDrive/ReQuEST/Results/Checkpoints/',\n",
        "        filename='{epoch:03d}',\n",
        "        every_n_epochs = 5,\n",
        "        save_top_k = -1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ne8VHOm83f8r"
      },
      "outputs": [],
      "source": [
        "class LitReQuEST(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "      super(LitReQuEST, self).__init__()\n",
        "      self.save_hyperparameters()\n",
        "      self.Model = hparams['Model']\n",
        "      self.tokenizer = hparams['tokenizer']\n",
        "      self.RQE_Loss_Func = nn.BCEWithLogitsLoss()\n",
        "      self.FreezeEnc = hparams['FreezeLayers'][0]\n",
        "      self.FreezeDec = hparams['FreezeLayers'][1]\n",
        "      self.Coefficient = hparams['Coefficient']\n",
        "      self.max_epochs = hparams['max_epochs']\n",
        "      self.warmup = hparams['warmup']\n",
        "      self.weight_decay = hparams['weight_decay']\n",
        "      self.num_train_batches = hparams['num_train_batches']\n",
        "      self.opt_epsilon = hparams['epsilon']\n",
        "      self.Micro_iter= 3                                                        # to have 3 micro iterations set this parameter to 3\n",
        "      self.max_norm = 0.1\n",
        "      self.generated_summaries = []\n",
        "      self.generated_tags = []\n",
        "      self.predicted_labels = []\n",
        "      self.sig = nn.Sigmoid()\n",
        "      self.automatic_optimization = False\n",
        "      self.opt1, self.opt2, self.opt3, self.opt4,\\\n",
        "       self.opt5 = self.configure_optimizers()                                  # opt1: Optimizer_RQE, opt2: Optimizer_Encoder\n",
        "                                                                                # opt3: Optimizer_Decoder, opt4: Optimizer_SUM,\n",
        "                                                                                # opt5: Optimizer_TG\n",
        "      self.scheduler_Encoder = self.configure_scheduler(self.opt2)\n",
        "      self.scheduler_Decoder = self.configure_scheduler(self.opt3)\n",
        "\n",
        "      rqe_metrics = MetricCollection([\n",
        "            Accuracy(task=\"binary\", num_classes=2),\n",
        "            F1Score(task=\"binary\", num_classes=2),\n",
        "            Precision(task=\"binary\", num_classes=2),\n",
        "            Recall(task=\"binary\", num_classes=2),\n",
        "        ])\n",
        "      rouge_keys = (\"rouge1\", \"rouge2\", \"rougeL\")\n",
        "      sum_metrics = MetricCollection([\n",
        "            ROUGEScore(rouge_keys=rouge_keys),\n",
        "            BERTScore(device=\"cuda\")\n",
        "        ])\n",
        "      tg_metrics = MetricCollection([\n",
        "            ROUGEScore(rouge_keys=rouge_keys),\n",
        "            BERTScore(device=\"cuda\")\n",
        "        ])\n",
        "\n",
        "      self.test_rqe_metrics = rqe_metrics.clone(prefix='test_')\n",
        "      self.test_sum_metrics = sum_metrics.clone(prefix='test_')\n",
        "      self.test_tg_metrics = tg_metrics.clone(prefix='test_')\n",
        "      self.train_rqe_metrics = rqe_metrics.clone(prefix='train_')\n",
        "      self.train_sum_metrics = sum_metrics.clone(prefix='train_')\n",
        "      self.train_tg_metrics = tg_metrics.clone(prefix='train_')\n",
        "      self.val_rqe_metrics = rqe_metrics.clone(prefix='val_')\n",
        "      self.val_sum_metrics = sum_metrics.clone(prefix='val_')\n",
        "      self.val_tg_metrics = tg_metrics.clone(prefix='val_')\n",
        "\n",
        "\n",
        "    def forward(self, EncoderRQE_input_ids = None,\n",
        "                EncoderRQE_attention = None,\n",
        "                EncoderSUM_input_ids = None,\n",
        "                EncoderSUM_attention_mask = None,\n",
        "                EncoderTG_input_ids = None,\n",
        "                EncoderTG_attention_mask = None,\n",
        "                DecoderSUM_input_ids = None,\n",
        "                DecoderSUM_attention_mask = None,\n",
        "                DecoderTG_input_ids = None,\n",
        "                DecoderTG_attention_mask = None,\n",
        "                output_attentions = None,\n",
        "                output_hidden_states = None,\n",
        "                encoder_outputs = None,\n",
        "                SUM_labels = None,\n",
        "                TG_labels = None,\n",
        "                past_key_values = None,\n",
        "                head_mask = None,\n",
        "                decoder_head_mask = None,\n",
        "                cross_attn_head_mask = None,\n",
        "                inputs_embeds = None,\n",
        "                decoder_inputs_embeds = None,\n",
        "                use_cache = None,\n",
        "                return_dict = None,\n",
        "                decoder_task = None):\n",
        "        return self.Model(EncoderRQE_input_ids, EncoderRQE_attention,\n",
        "                          EncoderSUM_input_ids, EncoderSUM_attention_mask,\n",
        "                          EncoderTG_input_ids, EncoderTG_attention_mask,\n",
        "                          DecoderSUM_input_ids, DecoderSUM_attention_mask,\n",
        "                          DecoderTG_input_ids, DecoderTG_attention_mask,\n",
        "                          output_attentions, output_hidden_states,\n",
        "                          encoder_outputs, SUM_labels, TG_labels,\n",
        "                          past_key_values, head_mask, decoder_head_mask,\n",
        "                          cross_attn_head_mask, inputs_embeds,\n",
        "                          decoder_inputs_embeds, use_cache, return_dict,\n",
        "                          decoder_task)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      Q1_input_ids, Q1_attention, Q1Q2_input_ids, Q1Q2_attention, Q1Tags_input_ids, Q1Tags_attention,\\\n",
        "        TG_decoder_input_ids, TG_decoder_attention_mask, SUM_decoder_input_ids, SUM_decoder_attention_mask,\\\n",
        "          GoldSummary_input_ids, GoldTags_input_ids, Pair_Labels = batch\n",
        "\n",
        "      self.Model.train()\n",
        "      self.Model.Freeze_Parameters(self.FreezeDec, self.FreezeEnc)\n",
        "\n",
        "      for iter in range(0, self.Micro_iter):\n",
        "          self.Model.zero_grad()\n",
        "          for opt in [self.opt1, self.opt2, self.opt3, self.opt4, self.opt5]:\n",
        "              opt.zero_grad()\n",
        "\n",
        "          Model_Output = self.Model(EncoderRQE_input_ids = Q1Q2_input_ids,\n",
        "                                    EncoderRQE_attention = Q1Q2_attention,\n",
        "                                    EncoderSUM_input_ids = Q1Tags_input_ids,\n",
        "                                    EncoderSUM_attention_mask = Q1Tags_attention,\n",
        "                                    EncoderTG_input_ids = Q1_input_ids,\n",
        "                                    EncoderTG_attention_mask = Q1_attention,\n",
        "                                    DecoderSUM_attention_mask = SUM_decoder_attention_mask,\n",
        "                                    DecoderTG_attention_mask = TG_decoder_attention_mask,\n",
        "                                    TG_labels = GoldTags_input_ids,\n",
        "                                    SUM_labels = GoldSummary_input_ids,\n",
        "                                    return_dict = False)\n",
        "\n",
        "          Summary, Tags, PLabel, SUM_Loss, TG_Loss = Model_Output\n",
        "          RQE_Loss = self.RQE_Loss_Func((PLabel.view(-1, PLabel.shape[-1])).squeeze(1), Pair_Labels.float())\n",
        "\n",
        "          if self.Micro_iter == 1:\n",
        "              Encoder_Loss =SUM_Loss + RQE_Loss + TG_Loss\n",
        "              self.manual_backward(Encoder_Loss, retain_graph = True)\n",
        "              torch.nn.utils.clip_grad_norm_(self.Model.parameters(), self.max_norm)\n",
        "              self.opt2.step()\n",
        "              self.opt3.step()\n",
        "              self.opt1.step()\n",
        "              self.opt4.step()\n",
        "              self.opt5.step()\n",
        "          else:\n",
        "              if iter==0:\n",
        "                  RQE_Loss = self.Coefficient['RQE3'] * RQE_Loss\n",
        "                  self.manual_backward(RQE_Loss, retain_graph = True)\n",
        "                  torch.nn.utils.clip_grad_norm_(self.Model.parameters(), self.max_norm)\n",
        "                  self.opt1.step()\n",
        "\n",
        "                  SUM_Loss = self.Coefficient['SUM3'] * SUM_Loss\n",
        "                  self.manual_backward(SUM_Loss, retain_graph = True)\n",
        "                  torch.nn.utils.clip_grad_norm_(self.Model.parameters(), self.max_norm)\n",
        "                  self.opt4.step()\n",
        "\n",
        "                  TG_Loss = self.Coefficient['TG3'] * TG_Loss\n",
        "                  self.manual_backward(TG_Loss, retain_graph = True)\n",
        "                  torch.nn.utils.clip_grad_norm_(self.Model.parameters(), self.max_norm)\n",
        "                  self.opt5.step()\n",
        "\n",
        "              if iter==1:\n",
        "                  Decoder_Loss = self.Coefficient['SUM2'] * SUM_Loss +\\\n",
        "                      self.Coefficient['TG2'] * TG_Loss\n",
        "                  self.manual_backward(Decoder_Loss, retain_graph = True)\n",
        "                  torch.nn.utils.clip_grad_norm_(self.Model.parameters(), self.max_norm)\n",
        "                  self.opt3.step()\n",
        "                  # self.scheduler_Decoder.step()\n",
        "\n",
        "              if iter==2:\n",
        "                  Encoder_Loss = self.Coefficient['SUM'] * SUM_Loss +\\\n",
        "                      self.Coefficient['RQE'] * RQE_Loss +\\\n",
        "                      self.Coefficient['TG'] * TG_Loss\n",
        "                  self.manual_backward(Encoder_Loss, retain_graph = True)\n",
        "                  torch.nn.utils.clip_grad_norm_(self.Model.parameters(), self.max_norm)\n",
        "                  self.opt2.step()\n",
        "                  # self.scheduler_Encoder.step()\n",
        "\n",
        "              if iter==0:\n",
        "                  self.log('train/rqe_loss', RQE_Loss.item(), on_step=True, on_epoch=True)\n",
        "                  self.log('train/sum_loss', SUM_Loss.item(), on_step=True, on_epoch=True)\n",
        "                  self.log('train/tg_loss', TG_Loss.item(), on_step=True, on_epoch=True)\n",
        "\n",
        "                  Q1_input_ids = torch.where(\n",
        "                      Q1_input_ids != -100,\n",
        "                      Q1_input_ids,\n",
        "                      self.Model.SUM.config.pad_token_id\n",
        "                      )\n",
        "                  Q1s = self.tokenizer.batch_decode(\n",
        "                      sequences = Q1_input_ids,\n",
        "                      skip_special_tokens = True,\n",
        "                      clean_up_tokenization_spaces = False\n",
        "                      )\n",
        "                  GoldSummary_input_ids = torch.where(\n",
        "                      GoldSummary_input_ids != -100,\n",
        "                      GoldSummary_input_ids,\n",
        "                      self.Model.SUM.config.pad_token_id\n",
        "                      )\n",
        "                  GoldSummaries = self.tokenizer.batch_decode(\n",
        "                      sequences = GoldSummary_input_ids,\n",
        "                      skip_special_tokens = True,\n",
        "                      clean_up_tokenization_spaces = False\n",
        "                      )\n",
        "                  GoldTags_input_ids = torch.where(\n",
        "                      GoldTags_input_ids != -100,\n",
        "                      GoldTags_input_ids,\n",
        "                      self.Model.SUM.config.pad_token_id\n",
        "                      )\n",
        "                  GoldTags = self.tokenizer.batch_decode(\n",
        "                      sequences = GoldTags_input_ids,\n",
        "                      skip_special_tokens = True,\n",
        "                      clean_up_tokenization_spaces = False\n",
        "                      )\n",
        "                  labels_predicted = torch.round(self.sig(PLabel))\n",
        "                  summary_decoded = self.tokenizer.batch_decode(\n",
        "                      sequences = torch.argmax(Summary, dim=-1),\n",
        "                      skip_special_tokens = True,\n",
        "                      clean_up_tokenization_spaces = False\n",
        "                      )\n",
        "                  tg_decoded = self.tokenizer.batch_decode(\n",
        "                      sequences = torch.argmax(Tags, dim=-1),\n",
        "                      skip_special_tokens = True,\n",
        "                      clean_up_tokenization_spaces = False\n",
        "                      )\n",
        "\n",
        "                  self.train_sum_metrics.update(summary_decoded, GoldSummaries)\n",
        "                  self.train_tg_metrics.update(tg_decoded, GoldTags)\n",
        "                  self.train_rqe_metrics.update(labels_predicted.squeeze(1), Pair_Labels)\n",
        "\n",
        "      output =  {\n",
        "          'Model_Output': Model_Output,\n",
        "          'RQE_Loss': RQE_Loss,\n",
        "          'SUM_Loss': SUM_Loss,\n",
        "          'TG_Loss': TG_Loss\n",
        "          }\n",
        "      return output\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "      Q1_input_ids, Q1_attention, Q1Q2_input_ids, Q1Q2_attention, Q1Tags_input_ids, Q1Tags_attention,\\\n",
        "        TG_decoder_input_ids, TG_decoder_attention_mask, SUM_decoder_input_ids, SUM_decoder_attention_mask,\\\n",
        "          GoldSummary_input_ids, GoldTags_input_ids, Pair_Labels = batch\n",
        "\n",
        "      summary_decoded = self.Model.SUM_generate2(\n",
        "        tokenizer = self.tokenizer,\n",
        "        input_ids = Q1Tags_input_ids,\n",
        "        _max_length = 140,\n",
        "        _min_length = 20,\n",
        "        _num_beams = 4,\n",
        "        _no_repeat_ngram_size = 3\n",
        "        )\n",
        "      tg_decoded = self.Model.TG_generate2(\n",
        "        tokenizer = self.tokenizer,\n",
        "        input_ids = Q1_input_ids,\n",
        "        _max_length = 30, #20,\n",
        "        _min_length = 7, #3,\n",
        "        _num_beams = 4,\n",
        "        _no_repeat_ngram_size = 3\n",
        "        )\n",
        "      labels_predicted = self.Model.RQE_predict(\n",
        "          input_ids = Q1Q2_input_ids,\n",
        "          attention_masks = Q1Q2_attention,\n",
        "          )\n",
        "      Q1_input_ids = torch.where(\n",
        "          Q1_input_ids != -100,\n",
        "          Q1_input_ids,\n",
        "          self.Model.SUM.config.pad_token_id\n",
        "          )\n",
        "      Q1s = self.tokenizer.batch_decode(\n",
        "          sequences = Q1_input_ids,\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "      GoldSummary_input_ids = torch.where(\n",
        "          GoldSummary_input_ids != -100,\n",
        "          GoldSummary_input_ids,\n",
        "          self.Model.SUM.config.pad_token_id\n",
        "          )\n",
        "      GoldSummaries = self.tokenizer.batch_decode(\n",
        "          sequences = GoldSummary_input_ids,\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "      GoldTags_input_ids = torch.where(\n",
        "          GoldTags_input_ids != -100,\n",
        "          GoldTags_input_ids,\n",
        "          self.Model.SUM.config.pad_token_id\n",
        "          )\n",
        "      GoldTags = self.tokenizer.batch_decode(\n",
        "          sequences = GoldTags_input_ids,\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "\n",
        "      self.test_sum_metrics.update(summary_decoded, GoldSummaries)\n",
        "      self.test_tg_metrics.update(tg_decoded, GoldTags)\n",
        "      self.test_rqe_metrics.update(labels_predicted.squeeze(1), Pair_Labels)\n",
        "\n",
        "      self.generated_summaries.append(summary_decoded)\n",
        "      self.generated_tags.append(tg_decoded)\n",
        "      self.predicted_labels.append(labels_predicted.squeeze(1).tolist())\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      Q1_input_ids, Q1_attention, Q1Q2_input_ids, Q1Q2_attention, Q1Tags_input_ids, Q1Tags_attention,\\\n",
        "        TG_decoder_input_ids, TG_decoder_attention_mask, SUM_decoder_input_ids, SUM_decoder_attention_mask,\\\n",
        "          GoldSummary_input_ids, GoldTags_input_ids, Pair_Labels = batch\n",
        "\n",
        "      self.Model.eval()\n",
        "      with torch.no_grad():\n",
        "        Model_Output = self.Model(EncoderRQE_input_ids = Q1Q2_input_ids,\n",
        "                          EncoderRQE_attention = Q1Q2_attention,\n",
        "                          EncoderSUM_input_ids = Q1Tags_input_ids,\n",
        "                          EncoderSUM_attention_mask = Q1Tags_attention,\n",
        "                          EncoderTG_input_ids = Q1_input_ids,\n",
        "                          EncoderTG_attention_mask = Q1_attention,\n",
        "                          DecoderSUM_attention_mask = SUM_decoder_attention_mask,\n",
        "                          DecoderTG_attention_mask = TG_decoder_attention_mask,\n",
        "                          TG_labels = GoldTags_input_ids,\n",
        "                          SUM_labels = GoldSummary_input_ids,\n",
        "                          return_dict = False)\n",
        "\n",
        "        Summary, Tags, PLabel, SUM_Loss, TG_Loss = Model_Output\n",
        "        RQE_Loss = self.RQE_Loss_Func((PLabel.view(-1, PLabel.shape[-1])).squeeze(1), Pair_Labels.float())\n",
        "\n",
        "\n",
        "      self.log('val/rqe_loss', RQE_Loss.item(), on_step=False, on_epoch=True)\n",
        "      self.log('val/sum_loss', SUM_Loss.item(), on_step=False, on_epoch=True)\n",
        "      self.log('val/tg_loss', TG_Loss.item(), on_step=False, on_epoch=True)\n",
        "\n",
        "      Q1_input_ids = torch.where(\n",
        "          Q1_input_ids != -100,\n",
        "          Q1_input_ids,\n",
        "          self.Model.SUM.config.pad_token_id\n",
        "          )\n",
        "      Q1s = self.tokenizer.batch_decode(\n",
        "          sequences = Q1_input_ids,\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "      GoldSummary_input_ids = torch.where(\n",
        "          GoldSummary_input_ids != -100,\n",
        "          GoldSummary_input_ids,\n",
        "          self.Model.SUM.config.pad_token_id\n",
        "          )\n",
        "      GoldSummaries = self.tokenizer.batch_decode(\n",
        "          sequences = GoldSummary_input_ids,\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "      GoldTags_input_ids = torch.where(\n",
        "          GoldTags_input_ids != -100,\n",
        "          GoldTags_input_ids,\n",
        "          self.Model.SUM.config.pad_token_id\n",
        "          )\n",
        "      GoldTags = self.tokenizer.batch_decode(\n",
        "          sequences = GoldTags_input_ids,\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "      labels_predicted = torch.round(self.sig(PLabel))\n",
        "      summary_decoded = self.tokenizer.batch_decode(\n",
        "          sequences = torch.argmax(Summary, dim=-1),\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "      tg_decoded = self.tokenizer.batch_decode(\n",
        "          sequences = torch.argmax(Tags, dim=-1),\n",
        "          skip_special_tokens = True,\n",
        "          clean_up_tokenization_spaces = False\n",
        "          )\n",
        "\n",
        "      self.val_sum_metrics.update(summary_decoded, GoldSummaries)\n",
        "      self.val_tg_metrics.update(tg_decoded, GoldTags)\n",
        "      self.val_rqe_metrics.update(labels_predicted.squeeze(1), Pair_Labels)\n",
        "\n",
        "      output =  {\n",
        "          'Model_Output': Model_Output,\n",
        "          'RQE_Loss': RQE_Loss,\n",
        "          'SUM_Loss': SUM_Loss,\n",
        "          'TG_Loss': TG_Loss\n",
        "          }\n",
        "      return output\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "      val_rqe_metrics = self.val_rqe_metrics.compute()\n",
        "      val_sum_metrics = self.val_sum_metrics.compute()\n",
        "      val_tg_metrics = self.val_tg_metrics.compute()\n",
        "\n",
        "      self.log_dict(val_rqe_metrics, prog_bar=False)\n",
        "      self.log('val/val_rouge1_sum(f)', val_sum_metrics['val_rouge1_fmeasure'])\n",
        "      self.log('val/val_rouge2_sum(f)', val_sum_metrics['val_rouge2_fmeasure'])\n",
        "      self.log('val/val_rougel_sum(f)', val_sum_metrics['val_rougeL_fmeasure'])\n",
        "      self.log('val/val_rouge1_sum(r)', val_sum_metrics['val_rouge1_recall'])\n",
        "      self.log('val/val_rouge2_sum(r)', val_sum_metrics['val_rouge2_recall'])\n",
        "      self.log('val/val_rougel_sum(r)', val_sum_metrics['val_rougeL_recall'])\n",
        "      self.log('val/val_rouge1_sum(p)', val_sum_metrics['val_rouge1_precision'])\n",
        "      self.log('val/val_rouge2_sum(p)', val_sum_metrics['val_rouge2_precision'])\n",
        "      self.log('val/val_rougel_sum(p)', val_sum_metrics['val_rougeL_precision'])\n",
        "      self.log('val/val_rouge1_tg(f)', val_tg_metrics['val_rouge1_fmeasure'])\n",
        "      self.log('val/val_rouge2_tg(f)', val_tg_metrics['val_rouge2_fmeasure'])\n",
        "      self.log('val/val_rougel_tg(f)', val_tg_metrics['val_rougeL_fmeasure'])\n",
        "      self.log('val/val_rouge1_tg(r)', val_tg_metrics['val_rouge1_recall'])\n",
        "      self.log('val/val_rouge2_tg(r)', val_tg_metrics['val_rouge2_recall'])\n",
        "      self.log('val/val_rougel_tg(r)', val_tg_metrics['val_rougeL_recall'])\n",
        "      self.log('val/val_rouge1_tg(p)', val_tg_metrics['val_rouge1_precision'])\n",
        "      self.log('val/val_rouge2_tg(p)', val_tg_metrics['val_rouge2_precision'])\n",
        "      self.log('val/val_rougel_tg(p)', val_tg_metrics['val_rougeL_precision'])\n",
        "      self.log('val/val_BS_score_sum(r)', val_sum_metrics['val_recall'].mean())\n",
        "      self.log('val/val_BS_score_sum(f)', val_sum_metrics['val_f1'].mean())\n",
        "      self.log('val/val_BS_score_sum(p)', val_sum_metrics['val_precision'].mean())\n",
        "      self.log('val/val_BS_score_tg(r)', val_tg_metrics['val_recall'].mean())\n",
        "      self.log('val/val_BS_score_tg(f)', val_tg_metrics['val_f1'].mean())\n",
        "      self.log('val/val_BS_score_tg(p)', val_tg_metrics['val_precision'].mean())\n",
        "\n",
        "      self.val_rqe_metrics.reset()\n",
        "      self.val_sum_metrics.reset()\n",
        "      self.val_tg_metrics.reset()\n",
        "\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "      train_rqe_metrics = self.train_rqe_metrics.compute()\n",
        "      train_sum_metrics = self.train_sum_metrics.compute()\n",
        "      train_tg_metrics = self.train_tg_metrics.compute()\n",
        "\n",
        "      self.log_dict(train_rqe_metrics, prog_bar=False)\n",
        "      self.log('train/train_rouge1_sum(f)', train_sum_metrics['train_rouge1_fmeasure'])\n",
        "      self.log('train/train_rouge2_sum(f)', train_sum_metrics['train_rouge2_fmeasure'])\n",
        "      self.log('train/train_rougel_sum(f)', train_sum_metrics['train_rougeL_fmeasure'])\n",
        "      self.log('train/train_rouge1_sum(r)', train_sum_metrics['train_rouge1_recall'])\n",
        "      self.log('train/train_rouge2_sum(r)', train_sum_metrics['train_rouge2_recall'])\n",
        "      self.log('train/train_rougel_sum(r)', train_sum_metrics['train_rougeL_recall'])\n",
        "      self.log('train/train_rouge1_sum(p)', train_sum_metrics['train_rouge1_precision'])\n",
        "      self.log('train/train_rouge2_sum(p)', train_sum_metrics['train_rouge2_precision'])\n",
        "      self.log('train/train_rougel_sum(p)', train_sum_metrics['train_rougeL_precision'])\n",
        "      self.log('train/train_rouge1_tg(f)', train_tg_metrics['train_rouge1_fmeasure'])\n",
        "      self.log('train/train_rouge2_tg(f)', train_tg_metrics['train_rouge2_fmeasure'])\n",
        "      self.log('train/train_rougel_tg(f)', train_tg_metrics['train_rougeL_fmeasure'])\n",
        "      self.log('train/train_rouge1_tg(r)', train_tg_metrics['train_rouge1_recall'])\n",
        "      self.log('train/train_rouge2_tg(r)', train_tg_metrics['train_rouge2_recall'])\n",
        "      self.log('train/train_rougel_tg(r)', train_tg_metrics['train_rougeL_recall'])\n",
        "      self.log('train/train_rouge1_tg(p)', train_tg_metrics['train_rouge1_precision'])\n",
        "      self.log('train/train_rouge2_tg(p)', train_tg_metrics['train_rouge2_precision'])\n",
        "      self.log('train/train_rougel_tg(p)', train_tg_metrics['train_rougeL_precision'])\n",
        "      self.log('train/train_BS_score_sum(r)', train_sum_metrics['train_recall'].mean())\n",
        "      self.log('train/train_BS_score_sum(f)', train_sum_metrics['train_f1'].mean())\n",
        "      self.log('train/train_BS_score_sum(p)', train_sum_metrics['train_precision'].mean())\n",
        "      self.log('train/train_BS_score_tg(r)', train_tg_metrics['train_recall'].mean())\n",
        "      self.log('train/train_BS_score_tg(f)', train_tg_metrics['train_f1'].mean())\n",
        "      self.log('train/train_BS_score_tg(p)', train_tg_metrics['train_precision'].mean())\n",
        "\n",
        "      self.train_rqe_metrics.reset()\n",
        "      self.train_sum_metrics.reset()\n",
        "      self.train_tg_metrics.reset()\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "      te_rqe_metrics = self.test_rqe_metrics.compute()\n",
        "      te_sum_metrics = self.test_sum_metrics.compute()\n",
        "      te_tg_metrics = self.test_tg_metrics.compute()\n",
        "\n",
        "      self.log_dict(te_rqe_metrics, prog_bar=False)\n",
        "      self.log('test/test_rouge1_sum(f)', te_sum_metrics['test_rouge1_fmeasure'])\n",
        "      self.log('test/test_rouge2_sum(f)', te_sum_metrics['test_rouge2_fmeasure'])\n",
        "      self.log('test/test_rougel_sum(f)', te_sum_metrics['test_rougeL_fmeasure'])\n",
        "      self.log('test/test_rouge1_sum(r)', te_sum_metrics['test_rouge1_recall'])\n",
        "      self.log('test/test_rouge2_sum(r)', te_sum_metrics['test_rouge2_recall'])\n",
        "      self.log('test/test_rougel_sum(r)', te_sum_metrics['test_rougeL_recall'])\n",
        "      self.log('test/test_rouge1_sum(p)', te_sum_metrics['test_rouge1_precision'])\n",
        "      self.log('test/test_rouge2_sum(p)', te_sum_metrics['test_rouge2_precision'])\n",
        "      self.log('test/test_rougel_sum(p)', te_sum_metrics['test_rougeL_precision'])\n",
        "      self.log('test/test_rouge1_tg(f)', te_tg_metrics['test_rouge1_fmeasure'])\n",
        "      self.log('test/test_rouge2_tg(f)', te_tg_metrics['test_rouge2_fmeasure'])\n",
        "      self.log('test/test_rougel_tg(f)', te_tg_metrics['test_rougeL_fmeasure'])\n",
        "      self.log('test/test_rouge1_tg(r)', te_tg_metrics['test_rouge1_recall'])\n",
        "      self.log('test/test_rouge2_tg(r)', te_tg_metrics['test_rouge2_recall'])\n",
        "      self.log('test/test_rougel_tg(r)', te_tg_metrics['test_rougeL_recall'])\n",
        "      self.log('test/test_rouge1_tg(p)', te_tg_metrics['test_rouge1_precision'])\n",
        "      self.log('test/test_rouge2_tg(p)', te_tg_metrics['test_rouge2_precision'])\n",
        "      self.log('test/test_rougel_tg(p)', te_tg_metrics['test_rougeL_precision'])\n",
        "      self.log('test/test_BS_score_sum(r)', te_sum_metrics['test_recall'].mean())\n",
        "      self.log('test/test_BS_score_sum(f)', te_sum_metrics['test_f1'].mean())\n",
        "      self.log('test/test_BS_score_sum(p)', te_sum_metrics['test_precision'].mean())\n",
        "      self.log('test/test_BS_score_tg(r)', te_tg_metrics['test_recall'].mean())\n",
        "      self.log('test/test_BS_score_tg(f)', te_tg_metrics['test_f1'].mean())\n",
        "      self.log('test/test_BS_score_tg(p)', te_tg_metrics['test_precision'].mean())\n",
        "\n",
        "      self.test_rqe_metrics.reset()\n",
        "      self.test_sum_metrics.reset()\n",
        "      self.test_tg_metrics.reset()\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      params_Enc = list(self.Model.SUM.model.encoder.layers[3:].parameters())\n",
        "      opt_BART_Encoder = torch.optim.AdamW(                                     #  1.  Optimizer for BART Encoder module only\n",
        "          params_Enc,\n",
        "          lr = self.Model.learning_rate_Encoder,\n",
        "          eps = self.opt_epsilon,\n",
        "          weight_decay=self.weight_decay,\n",
        "          )\n",
        "\n",
        "\n",
        "      params_Dec = list(self.Model.SUM.model.decoder.layers[0:3].parameters()) +\\\n",
        "                   list(self.Model.SUM.model.decoder.embed_positions.parameters()) +\\\n",
        "                   list(self.Model.SUM.model.decoder.layernorm_embedding.parameters()) +\\\n",
        "                   list(self.Model.SUM.model.decoder.embed_tokens.parameters())\n",
        "      opt_BART_Decoder = torch.optim.AdamW(                                     #  2.  Optimizer for BART shared Decoder layers only\n",
        "          params_Dec,\n",
        "          lr = self.Model.learning_rate_Decoder,\n",
        "          eps = self.opt_epsilon,\n",
        "          weight_decay=self.weight_decay,\n",
        "          )\n",
        "\n",
        "\n",
        "      params_RQE = list(self.Model.RQE.lm_head.parameters())\n",
        "      opt_RQE = torch.optim.AdamW(                                              #  3.  Optimizer for RQE module only\n",
        "          params_RQE,\n",
        "          lr = self.Model.learning_rate_RQE,\n",
        "          eps = self.opt_epsilon,\n",
        "          weight_decay=self.weight_decay,\n",
        "          )\n",
        "\n",
        "\n",
        "      paramsSUM = list(self.Model.SUM.lm_head.parameters()) +\\\n",
        "                  list(self.Model.SUM.model.decoder.layers[3:].parameters())\n",
        "      opt_SUM = torch.optim.AdamW(                                              #  4.  Optimizer for Summarization module only\n",
        "          paramsSUM,\n",
        "          lr = self.Model.learning_rate_SUM,\n",
        "          eps = self.opt_epsilon,\n",
        "          weight_decay=self.weight_decay,\n",
        "          )\n",
        "\n",
        "\n",
        "      paramsTG = list(self.Model.TG.lm_head.parameters()) +\\\n",
        "                 list(self.Model.TG.model.decoder.layers[3:].parameters())\n",
        "      opt_TG = torch.optim.AdamW(                                               #  5.  Optimizer for Tag generation module only\n",
        "          paramsTG,\n",
        "          lr = self.Model.learning_rate_TG,\n",
        "          eps = self.opt_epsilon,\n",
        "          weight_decay=self.weight_decay,\n",
        "          )\n",
        "\n",
        "      self.Model.Freeze_Parameters(self.FreezeDec, self.FreezeEnc)\n",
        "      return opt_RQE, opt_BART_Encoder, opt_BART_Decoder, opt_SUM, opt_TG\n",
        "\n",
        "\n",
        "    def configure_scheduler(self, optimizer):\n",
        "      total_steps = self.num_train_batches * self.max_epochs\n",
        "      scheduler = get_linear_schedule_with_warmup(\n",
        "          optimizer,\n",
        "          num_warmup_steps = self.warmup,\n",
        "          num_training_steps = total_steps\n",
        "          )\n",
        "      return scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw7pL_B2impy"
      },
      "source": [
        "# ðŸŒž **Hyper-Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4vzKXeiimpy"
      },
      "outputs": [],
      "source": [
        "BARTtokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\", lowercase = False)\n",
        "BARTmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "hparams_Main = {\n",
        "    'embed_size' : BARTmodel.config.d_model,\n",
        "    'DO_r' : 0.1,\n",
        "    'lr_RQE' :2e-5,\n",
        "    'lr_SUM' : 2e-5,\n",
        "    'lr_TG' : 2e-5,\n",
        "    'lr_Encoder' : 2e-5,\n",
        "    'lr_Decoder' : 2e-5,\n",
        "    'Dimensions' : [48]                                                          # [n1, n2, n3, ....]\n",
        "    }\n",
        "Main_Arch_Obj = Main_Architecture(BARTmodel.config,hparams_Main)\n",
        "max_epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "hparams_MyModel = {\n",
        "    'Model': Main_Arch_Obj,\n",
        "    'tokenizer': BARTtokenizer,\n",
        "    'label_smoothing':  0.1,\n",
        "    'FreezeLayers':[[0, 0, 0, 1, 1, 1],                                          # (Encoder) 0: Freeze, 1: Unfreeze\n",
        "                    [1, 1, 1, 1, 1, 1]],                                         # (Decoder) 0: Freeze, 1: Unfreeze\n",
        "    'Coefficient': {'RQE': 0.7, 'SUM':0.2, 'TG':0.1,\n",
        "                    'SUM2':1, 'TG2':1,\n",
        "                    'RQE3':1, 'SUM3':1, 'TG3':1},\n",
        "    'max_epochs': max_epochs,\n",
        "    'warmup': 0.0,\n",
        "    'epsilon': 1e-8,\n",
        "    'weight_decay': 0.01,\n",
        "    }\n",
        "\n",
        "print(\"Maximum position embeddings: \", BARTmodel.config.max_position_embeddings)\n",
        "print(\"Size of embeddings: \" , BARTmodel.config.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loBo8Zowimps"
      },
      "source": [
        "# ðŸŒž **Data preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTEGM7MK8QwP"
      },
      "source": [
        ">## âœ¨ **Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P8gYcYq3impu"
      },
      "outputs": [],
      "source": [
        "def Preprocess_Text (txt, txttype='S'):\n",
        "    if (txttype == 'T'):\n",
        "        txt = \", \".join(txt)\n",
        "        filtered_sentence = txt.replace('-', ' ')\n",
        "    else:\n",
        "        txt = txt.replace('-', ' ')\n",
        "        txt = txt.replace('\\n', ' ')\n",
        "        txt = txt.replace('â€œ', ' ')\n",
        "        txt = txt.replace('â€', ' ')\n",
        "        txt = re.sub(r'http\\S+', '', txt, flags=re.MULTILINE)\n",
        "        filtered_sentence = txt.translate(str.maketrans('', '', string.punctuation))\n",
        "    return filtered_sentence\n",
        "\n",
        "def Preprocess_Data(Data):\n",
        "    Check_list = ['long_text', 'short_text',\n",
        "                  'long_text_title', 'long_text_tags', 'short_text_tags'\n",
        "                  ]\n",
        "    for index,row in Data.iterrows():\n",
        "        for i, C in enumerate(Check_list):\n",
        "            txttype = 'T' if i>2 else 'S'\n",
        "            filtered_sentence = Preprocess_Text(row[C], txttype)\n",
        "            Data.at[index, C] = filtered_sentence\n",
        "    return Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-C3dW2NSpJr"
      },
      "outputs": [],
      "source": [
        "data_tr = f\"/content/drive/MyDrive/ReQuEST/Data/TrainData.pkl\"\n",
        "MyData_tr = pd.read_pickle(data_tr)\n",
        "MyData_tr['is_duplicate'] = [1 if row == 'Entailed' else 0 for row in MyData_tr['is_duplicate']]\n",
        "\n",
        "data_te = f\"/content/drive/MyDrive/ReQuEST/Data/TestData.pkl\"\n",
        "MyData_te = pd.read_pickle(data_te)\n",
        "MyData_te['is_duplicate'] = [1 if row == 'Entailed' else 0 for row in MyData_te['is_duplicate']]\n",
        "\n",
        "MyData_tr = Preprocess_Data(MyData_tr)\n",
        "MyData_te = Preprocess_Data(MyData_te)\n",
        "\n",
        "display(MyData_tr)\n",
        "display(MyData_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1NRcGurimpz"
      },
      "source": [
        ">## âœ¨ **Custom Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kII9Rtlzb0OG"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, pad_token_id):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_tID = pad_token_id\n",
        "        self.labels = [0, 1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.loc[idx]\n",
        "        label = self.labels.index(row[8])\n",
        "\n",
        "        # ================ Tokenization for RQE ==============\n",
        "        RQE_input_text = f\"Question1: {row[0]} </s></s> Question2: {row[1]}\"\n",
        "        Q1Q2_Tokenized = self.tokenizer(RQE_input_text, max_length = 256, padding = 'max_length')\n",
        "\n",
        "        # =========== Tokenization for Tag Generation ========\n",
        "        Q1_Tokenized = self.tokenizer(row[0], max_length = 140, padding='max_length')\n",
        "        Q1_Tags_Tokenized = self.tokenizer(row[2], max_length = 140, padding='max_length')\n",
        "        Q1_Tags_input_ids = [-100 if t == self.pad_tID else t for t in Q1_Tags_Tokenized[\"input_ids\"]]\n",
        "\n",
        "        # =========== Tokenization for Summarization =========\n",
        "        if (label == 0):\n",
        "          SUM_input_text = f\"Question: {row[0]} </s></s> Query: {row[2]}\"\n",
        "          Q1Tags_Tokenized = self.tokenizer(SUM_input_text, max_length = 140, padding='max_length')\n",
        "          Q1Title_Tokenized = self.tokenizer(row[4], max_length = 140, padding='max_length')\n",
        "          Summary_input_ids = [-100 if t == self.pad_tID else t for t in Q1Title_Tokenized[\"input_ids\"]]\n",
        "          SUM_decoder_attention_mask =  Q1Title_Tokenized[\"attention_mask\"]\n",
        "          SUM_decoder_input_ids = Q1Title_Tokenized[\"input_ids\"]\n",
        "        else:\n",
        "          SUM_input_text = f\"Question: {row[0]} </s></s> Query: {row[3]}\"\n",
        "          Q1Tags_Tokenized = self.tokenizer(SUM_input_text, max_length = 140, padding='max_length')\n",
        "          Q2_Tokenized = self.tokenizer(row[1], max_length = 140, padding='max_length')\n",
        "          Summary_input_ids = [-100 if t == self.pad_tID else t for t in Q2_Tokenized[\"input_ids\"]]\n",
        "          SUM_decoder_attention_mask =  Q2_Tokenized[\"attention_mask\"]\n",
        "          SUM_decoder_input_ids = Q2_Tokenized[\"input_ids\"]\n",
        "\n",
        "\n",
        "        return torch.tensor(Q1_Tokenized['input_ids']),\\\n",
        "            torch.tensor(Q1_Tokenized['attention_mask']),\\\n",
        "            torch.tensor(Q1Q2_Tokenized['input_ids']),\\\n",
        "            torch.tensor(Q1Q2_Tokenized['attention_mask']),\\\n",
        "            torch.tensor(Q1Tags_Tokenized['input_ids']),\\\n",
        "            torch.tensor(Q1Tags_Tokenized['attention_mask']),\\\n",
        "            torch.tensor(Q1_Tags_Tokenized[\"input_ids\"]),\\\n",
        "            torch.tensor(Q1_Tags_Tokenized['attention_mask']),\\\n",
        "            torch.tensor(SUM_decoder_input_ids),\\\n",
        "            torch.tensor(SUM_decoder_attention_mask),\\\n",
        "            torch.tensor(Summary_input_ids),\\\n",
        "            torch.tensor(Q1_Tags_input_ids),\\\n",
        "            torch.tensor(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ_iXtflAyCq"
      },
      "source": [
        ">## âœ¨ **Custom Data Module**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aUDRyhoY87e5"
      },
      "outputs": [],
      "source": [
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self,\n",
        "                 MyData_tr,\n",
        "                 MyData_te,\n",
        "                 tokenizer,\n",
        "                 pad_token_id,\n",
        "                 batch_size = 16,\n",
        "                 split_ratio = [0.8, 0.1, 0.1],\n",
        "                 stage=None):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.MyData_tr = MyData_tr\n",
        "        self.MyData_te = MyData_te\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_id = pad_token_id\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = CustomDataset(self.MyData_tr, self.tokenizer, self.pad_id)\n",
        "        self.test_dataset = CustomDataset(self.MyData_te, self.tokenizer, self.pad_id)\n",
        "        self.val_dataset = CustomDataset(self.MyData_te, self.tokenizer, self.pad_id)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler = RandomSampler(self.train_dataset, generator= torch.Generator().manual_seed(42)),\n",
        "            batch_size = self.batch_size,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            sampler = SequentialSampler(self.val_dataset),\n",
        "            batch_size = self.batch_size,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            sampler = SequentialSampler(self.test_dataset),\n",
        "            batch_size = self.batch_size,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uzNpDTjbFrDI"
      },
      "outputs": [],
      "source": [
        "DataModule = CustomDataModule(\n",
        "    MyData_tr, MyData_te,\n",
        "    BARTtokenizer,\n",
        "    BARTmodel.config.pad_token_id,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MTuFCN6l8sGp"
      },
      "outputs": [],
      "source": [
        "DataModule.setup()\n",
        "num_train_batches = len(DataModule.train_dataloader())\n",
        "hparams_MyModel['num_train_batches'] = num_train_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clBlf-aUimpy"
      },
      "source": [
        "# ðŸŒž **Model Compile**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5bM_SXgimpz"
      },
      "outputs": [],
      "source": [
        "MyModel = LitReQuEST(hparams_MyModel)\n",
        "print(MyModel)\n",
        "\n",
        "logger = TensorBoardLogger(\"/content/drive/MyDrive/ReQuEST/Results/logs\", name=\"ReQuEST_Logs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWxUNLNLMqTR"
      },
      "source": [
        "# ðŸŒž **Training Phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-v_G8DpMmW3"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger = logger,\n",
        "    max_epochs = hparams_MyModel['max_epochs'],\n",
        "    log_every_n_steps = 1,\n",
        "    num_sanity_val_steps = 0,\n",
        "    callbacks = [OverrideEpochStepCallback(), checkpoint_callback],\n",
        "    default_root_dir=\"/content/drive/MyDrive/ReQuEST/Results/Checkpoints/\",\n",
        "    )\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/ReQuEST/\n",
        "\n",
        "trainer.fit(\n",
        "    MyModel,\n",
        "    datamodule=DataModule,\n",
        "    # ckpt_path=\"/content/drive/MyDrive/ReQuEST/Results/Checkpoints/MyModel_checkpoints10.ckpt\",\n",
        "    )\n",
        "\n",
        "trainer.save_checkpoint(\n",
        "    f\"/content/drive/MyDrive/ReQuEST/Results/Checkpoints/MyModel_checkpoints{hparams_MyModel['max_epochs']}.ckpt\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOGc55v8pzXA"
      },
      "source": [
        "# ðŸŒž **Test Phase**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = trainer.test(MyModel, datamodule=DataModule)\n",
        "\n",
        "print(MyModel.generated_summaries)\n",
        "print(MyModel.generated_tags)\n",
        "print(MyModel.predicted_labels)"
      ],
      "metadata": {
        "id": "5H6jDftYfrYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "azuZ4HNximpr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}